# Pegasos Kernel SVM Implementation

A deterministic implementation of the Pegasos algorithm for training kernel SVMs from scratch.

## üß† Theory: The Math Behind SVMs

SVMs are mathematically intense. There are two main approaches:

1. **Primal SVM (Pegasos):** Uses sub-gradient descent directly on the hinge loss objective.
2. **Dual SVM (SMO):** Uses Lagrange multipliers and the KKT conditions to solve the dual optimization problem.

### The Kernel Trick
Instead of computing transformations to higher dimensions explicitly, kernels compute the dot product in that space directly:
- **Linear:** `K(x, y) = x ¬∑ y`
- **RBF:** `K(x, y) = exp(-||x - y||¬≤ / 2œÉ¬≤)`

### Pegasos Update Rules (Deterministic)
The algorithm I implemented:
1. **Learning Rate:** `Œ∑_t = 1 / (Œª * t)` (decreases over time)
2. **Margin Check:** If `y_i * f(x_i) < 1` (margin violated):
   - `Œ±_i ‚Üê (1 - 1/t) * Œ±_i + Œ∑_t * y_i`
   - `b ‚Üê b + Œ∑_t * y_i`

> **Note:** The original Deep-ML description uses `Œ±_i ‚Üê Œ±_i + Œ∑_t(y_i - Œª*Œ±_i)`, but my solution simplifies the decay term to `(1 - 1/t)` which is equivalent to `(1 - Œ∑_t * Œª)`. Both achieve the same result.

---

## üí° Key Learnings & Insights

This was by far the **hardest concept mathematically** for me to understand.

- **Dual vs. Primal:** The SMO (Dual) method has complex update rules involving error terms (`E_i = f(x_i) - y_i`) and kernel similarities.
- **Lagrangian Multipliers:** Required for solving constrained optimization, a concept heavily used in physics too.
- **Recommended Resource:** [An Idiot's Guide to SVMs (MIT)](https://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf) ‚Äî An excellent resource, though don't be fooled by the title; this requires serious mathematical effort!

---
*Solved as part of my deep-learning journey on Deep-ML.*
